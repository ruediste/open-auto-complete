{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum[exporters] onnxruntime-openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='Qwen/Qwen2.5-Coder-0.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model {model_name} data/onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/onnx\")\n",
    "model = ORTModelForCausalLM.from_pretrained(\"data/onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "session=model.model\n",
    "print(session.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR /tmp/src/protobuf/src/google/protobuf/message_lite.cc:402] onnx.ModelProto exceeded maximum protobuf size of 2GB: 2521471831\n"
     ]
    },
    {
     "ename": "InvalidProtobuf",
     "evalue": "[ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Protobuf serialization failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidProtobuf\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer\n\u001b[0;32m----> 2\u001b[0m optimized_model \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/onnx/model.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m optimized_model\u001b[38;5;241m.\u001b[39mconvert_float_to_float16()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# optimized_model.save_model_to_file(\"bert_fp16.onnx\")\u001b[39;00m\n",
      "File \u001b[0;32m~/git/open-auto-complete/fine-tune/.venv/lib/python3.12/site-packages/onnxruntime/transformers/optimizer.py:381\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(input, model_type, num_heads, hidden_size, optimization_options, opt_level, use_gpu, only_onnxruntime, verbose, provider)\u001b[0m\n\u001b[1;32m    366\u001b[0m     temp_model_path \u001b[38;5;241m=\u001b[39m optimize_by_onnxruntime(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    368\u001b[0m         use_gpu\u001b[38;5;241m=\u001b[39muse_gpu,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m         save_as_external_data\u001b[38;5;241m=\u001b[39mhas_external_data_file,\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m opt_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# basic optimizations (like constant folding and cast elimination) are not specified to execution provider.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# Note that use_gpu=False might cause extra Cast nodes for float16 model since most operators does not support float16 in CPU.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Sometime, use_gpu=True might cause extra memory copy nodes when some operators are supported only in CPU.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# We might need remove GPU memory copy nodes as preprocess of optimize_by_fusion if they cause no matching in fusion.\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     temp_model_path \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_by_onnxruntime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimized_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimized_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_as_external_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_external_data_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_onnxruntime \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m temp_model_path:\n\u001b[1;32m    393\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify a positive value for opt_level when only_onnxruntime is True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/open-auto-complete/fine-tune/.venv/lib/python3.12/site-packages/onnxruntime/transformers/optimizer.py:206\u001b[0m, in \u001b[0;36moptimize_by_onnxruntime\u001b[0;34m(onnx_model, use_gpu, optimized_model_path, opt_level, disabled_optimizers, verbose, save_as_external_data, external_data_filename, external_data_file_threshold, provider, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Inference session is only used to optimize the model.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx_model\u001b[38;5;241m.\u001b[39mSerializeToString() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(onnx_model, ModelProto) \u001b[38;5;28;01melse\u001b[39;00m onnx_model\n\u001b[0;32m--> 206\u001b[0m \u001b[43monnxruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(optimized_model_path) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(optimized_model_path)\n\u001b[1;32m    209\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave optimized model by onnxruntime to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, optimized_model_path)\n",
      "File \u001b[0;32m~/git/open-auto-complete/fine-tune/.venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/git/open-auto-complete/fine-tune/.venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:537\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    534\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[0;31mInvalidProtobuf\u001b[0m: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Protobuf serialization failed."
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers import optimizer\n",
    "optimized_model = optimizer.optimize_model('data/onnx/model.onnx')\n",
    "optimized_model.convert_float_to_float16()\n",
    "# optimized_model.save_model_to_file(\"bert_fp16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens: 120 Output token count: 20 Time: 1681.2796592712402ms Token/s 83.26981201694998\n",
      "Response: log(\"Hello World\");\n",
      "// Log Hello World\n",
      "console.log(\"Hello World\");\n",
      "// Log Hello World\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "fim_prefix_id = tokenizer.convert_tokens_to_ids(\"<|fim_prefix|>\")\n",
    "fim_suffix_id = tokenizer.convert_tokens_to_ids(\"<|fim_suffix|>\")\n",
    "fim_middle_id = tokenizer.convert_tokens_to_ids(\"<|fim_middle|>\")\n",
    "fim_pad_id = tokenizer.convert_tokens_to_ids(\"<|fim_pad|>\")\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def generate_response(prefix, suffix):\n",
    "    start = time.time()\n",
    "    # Tokenize all prefixes and suffixes together\n",
    "    prefix_ids = tokenizer(prefix, add_special_tokens=False)[\"input_ids\"]\n",
    "    suffix_ids = tokenizer(suffix, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # Combine the IDs\n",
    "    inputs = {'input_ids': torch.tensor([[fim_prefix_id] + prefix_ids + [fim_suffix_id] + suffix_ids +[fim_middle_id]], dtype=torch.int64)}\n",
    "    inputs['attention_mask']=torch.tensor([[1]*inputs['input_ids'].shape[1]], dtype=torch.int64)\n",
    "\n",
    "\n",
    "    input_token_count=inputs['input_ids'].shape[1]\n",
    "    \n",
    "    outputs = model.generate(**inputs,max_new_tokens=20,eos_token_id=[tokenizer.eos_token_id,fim_pad_id,fim_suffix_id ] )\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    output_token_count=outputs.shape[-1]\n",
    "    print(\"Total input tokens: {} Output token count: {} Time: {}ms Token/s {}\".format(input_token_count, output_token_count, 1000*(time.time()-start), ( output_token_count+input_token_count)/(time.time()-start)))\n",
    "    return tokenizer.decode(outputs[0])\n",
    "# Example usage\n",
    "response = generate_response('print(\"Total input tokens: {} Output token count:'*10+'\\n// Log Hello World\\nconsole.','')\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
