services:
  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    restart: unless-stopped
    volumes:
      - ./data/webui:/app/backend/data
    environment:
      - WEBUI_AUTH=False
      - WEBUI_NAME=Local
      - OLLAMA_BASE_URL=http://ollama-server:11434
    ports:
      - 8081:8080

  ollama-server:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ollama-server
    restart: unless-stopped
    ports:
      - 11434:11434
    devices:
      - /dev/dri
    volumes:
      - ./data/ollama:/root/.ollama/models
      - ./run.sh:/run.sh
      - ../fine-tune/data:/root/data
    environment:
      - DEVICE=Arc
    shm_size: 16g
    command: ["/run.sh"]

  vllm-server:
    image: vllm-openvino-env
    container_name: vllm-server
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ../fine-tune/data:/models
    ports:
      - 8000:8000
    ipc: host
    command: "vllm serve Qwen/Qwen2.5-Coder-0.5B --dtype auto --api-key token-abc123 --task generate"
    # command: "vllm serve /models/test "
