{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"ruediste/codeparrot-github-code-10G\", \"cs\", split=\"train\", streaming=True )\n",
    "# dataset=load_dataset('parquet',data_dir=\"data/dataset\", streaming=True)['train']\n",
    "dataset=load_dataset('parquet',data_files=[\"data/data.parquet\"], streaming=True)['train']\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('data/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generateExamples(rows):\n",
    "    prefixes=[]\n",
    "    suffixes=[]\n",
    "    completions=[]\n",
    "    paths=[]\n",
    "    languages=[]\n",
    "    chunkSize=1000    \n",
    "    for i,code in enumerate(rows[\"code\"]):\n",
    "        base=0;\n",
    "        while base < len(code):\n",
    "            for completionSize in [0,5,10,20,100,200,400]:\n",
    "              idx=random.randint(base, min(len(code),base+chunkSize))\n",
    "              prefixes.append(code[max(0,idx-200):idx])\n",
    "              completions.append(code[idx:idx+completionSize])\n",
    "              suffixes.append(code[idx+completionSize:idx+completionSize+10])\n",
    "              paths.append(rows['path'][i])\n",
    "              languages.append(rows['language'][i])\n",
    "            base+=chunkSize\n",
    "    return {\"prefix\":prefixes, \"suffix\":suffixes, \"completion\":completions, \"path\":paths, \"language\": languages}\n",
    "\n",
    "ds=dataset.shuffle(seed=42).map(lambda rows:generateExamples(rows), batched=True, remove_columns=['code','size','license','repo_name'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    num_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pad = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "mask = tokenizer.convert_tokens_to_ids(\"<|mask|>\")\n",
    "cls = tokenizer.convert_tokens_to_ids(\"<|cls|>\")\n",
    "languageTokens={\n",
    "    'cs': tokenizer.convert_tokens_to_ids(\"<|cs|>\"),\n",
    "    'css': tokenizer.convert_tokens_to_ids(\"<|css|>\"),\n",
    "    'ts': tokenizer.convert_tokens_to_ids(\"<|ts|>\"),\n",
    "};\n",
    "languageIds={\n",
    "    'cs': 0,\n",
    "    'ts': 1,\n",
    "    'css': 2,\n",
    "};\n",
    "\n",
    "predictionTokens=5\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize all prefixes and suffixes together\n",
    "    prefix_ids = tokenizer(examples[\"prefix\"], add_special_tokens=False, split_special_tokens=True)[\"input_ids\"]\n",
    "    suffix_ids = tokenizer(examples[\"suffix\"], add_special_tokens=False,split_special_tokens=True)[\"input_ids\"]\n",
    "    completion_ids = tokenizer(examples[\"completion\"], add_special_tokens=False, split_special_tokens=True)[\"input_ids\"]\n",
    "    \n",
    "    # Combine the IDs for each example in the batch\n",
    "    # [languageTokens[lang]]\n",
    "    input_ids = [\n",
    "       prefix + [mask]*predictionTokens + suffix \n",
    "       for prefix, suffix, lang in zip(prefix_ids, suffix_ids, examples[\"language\"])\n",
    "    ]\n",
    "\n",
    "    # Create labels, replacing prefix and suffix with -100\n",
    "    label_ids =  [\n",
    "       prefix + completion[:predictionTokens]+[pad]*(max(0,predictionTokens-len(completion))) + suffix \n",
    "       for prefix,completion, suffix, lang in zip(prefix_ids, completion_ids, suffix_ids, examples[\"language\"])\n",
    "    ]\n",
    "\n",
    "    attention_mask = [[1] * len(ids) for ids in input_ids]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": label_ids,\n",
    "        \"attention_mask\":attention_mask,\n",
    "        \"token_type_ids\": [[languageIds[lang]] * len(ids) for ids,lang in zip(input_ids,examples[\"language\"])]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = ds.shuffle(seed=42).map(tokenize_function, batched=True,batch_size=10,remove_columns=['path','language','prefix','suffix','completion'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[872, 330, 391, 19, 561, 1034, 19, 1068, 32, 146, 146, 827, 1, 1, 1, 1, 1, 854, 4588, 84, 19, 59, 78]\n",
      "[872, 330, 391, 19, 561, 1034, 19, 1068, 32, 146, 146, 827, 863, 88, 0, 0, 0, 854, 4588, 84, 19, 59, 78]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "[872, 330, 391, 19, 561, 1034, 19, 1068, 32, 146, 146, 827, 1155, 4588, 84, 19, 3513, 19, 91, 21, 146, 96, 304, 313, 3434, 88, 1, 1, 1, 1, 1, 1117, 146, 183]\n",
      "[872, 330, 391, 19, 561, 1034, 19, 1068, 32, 146, 146, 827, 1155, 4588, 84, 19, 3513, 19, 91, 21, 146, 96, 304, 313, 3434, 88, 88, 2139, 88, 0, 0, 1117, 146, 183]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "34\n",
      "34\n",
      "34\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cs': 3, 'css': 5, 'ts': 4}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ex in tokenized_dataset.take(2):\n",
    "    print(ex['input_ids'])\n",
    "    print(ex['labels'])\n",
    "    print(ex['token_type_ids'])\n",
    "    print(len(ex['input_ids']))\n",
    "    print(len(ex['labels']))\n",
    "    print(len(ex['attention_mask']))\n",
    "    print(len(ex['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForMaskedLM, BertConfig\n",
    "\n",
    "config=BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    hidden_size=512, \n",
    "    num_hidden_layers=4, \n",
    "    num_attention_heads=8, \n",
    "    intermediate_size=2048, \n",
    "    max_position_embeddings=2048,\n",
    "    token_type_ids=3,\n",
    "    )\n",
    "model=BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16488840 || all params: 16488840 || trainable: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param}%\"\n",
    "    )\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitds=tokenized_dataset.train_test_split(2000)\n",
    "splitds={}\n",
    "splitds['test']=tokenized_dataset.take(2000)\n",
    "splitds['train']=tokenized_dataset.skip(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Union\n",
    "from transformers import TrainingArguments,Trainer,EvalPrediction\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import time\n",
    "\n",
    "\n",
    "def padToLength(list,length, padding):\n",
    "    result=list[:length]\n",
    "    return result + [padding]*(length-len(result));\n",
    "\n",
    "class MyDataCollator:\n",
    "    def __call__(self, features) :\n",
    "        max_length = max([len(feature['input_ids']) for feature in features])\n",
    "        max_length=16*math.ceil(max_length/16)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor([padToLength(feature['input_ids'],max_length, tokenizer.pad_token_id ) for feature in features], dtype=torch.int64),\n",
    "            \"labels\": torch.tensor([padToLength(feature['labels'],max_length, -100 ) for feature in features], dtype=torch.int64),\n",
    "            \"attention_mask\":torch.tensor([padToLength(feature['attention_mask'],max_length, 0 ) for feature in features], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=2,\n",
    "    output_dir=\"data/check\",\n",
    "    overwrite_output_dir=True,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30000,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    eval_on_start=True,\n",
    "    max_steps=2**30\n",
    "    #  gradient_checkpointing=True,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    )\n",
    "\n",
    "class Scheduler:\n",
    "    def __init__(self):\n",
    "        self.factor=1;\n",
    "        self.last_loss=None\n",
    "        self.last_time=time.time()\n",
    "        self.metrics=None\n",
    "        self.time_elapsed=False\n",
    "\n",
    "    def __call__(self, step: int):\n",
    "        if self.last_loss is None and self.metrics is not None:\n",
    "            self.last_loss=self.metrics['eval_loss']\n",
    "            \n",
    "        if self.time_elapsed:\n",
    "            if self.metrics is not None:\n",
    "                loss=self.metrics['eval_loss']\n",
    "                if loss/self.last_loss>0.95:\n",
    "                    # loss fell too slow, reduce lr\n",
    "                    print(\"LR Scheduler: reduce LR in step \"+str(step))\n",
    "                    self.factor*=0.8;\n",
    "                self.last_loss=loss;\n",
    "                self.last_time=time.time();\n",
    "                self.time_elapsed=False\n",
    "        else:\n",
    "            if time.time()-self.last_time > 30*60:\n",
    "                self.time_elapsed=True\n",
    "                # clear the metrics and wait for the next fresh evaluation\n",
    "                self.metrics=None\n",
    "        return self.factor;\n",
    "\n",
    "scheduler=Scheduler()\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def create_scheduler(self,num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n",
    "        self.lr_scheduler=LambdaLR(optimizer, scheduler)\n",
    "        return self.lr_scheduler\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> Dict[str, float]:\n",
    "        metrics= super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        scheduler.metrics=metrics\n",
    "        return metrics\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=splitds[\"train\"],\n",
    "    eval_dataset=splitds[\"test\"],\n",
    "    data_collator=MyDataCollator(),\n",
    "    processing_class=tokenizer,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "trainer.save_model('data/bert1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(e: EvalPrediction):\n",
    "    # print('compute metrics '+str(e.label_ids))\n",
    "    res=e.predictions.argmax(-1)\n",
    "    # print(e.label_ids)\n",
    "    for prediction,labels in zip(res,e.label_ids):\n",
    "        print(\"===***\")\n",
    "        print(tokenizer.decode(prediction, skip_special_tokens=True,))\n",
    "        print(\"---\")\n",
    "        print(tokenizer.decode(list(filter(lambda x: x != -100, labels)), skip_special_tokens=True,))\n",
    "        print(\"===+++\")\n",
    "\n",
    "    return {'eval_loss':1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
